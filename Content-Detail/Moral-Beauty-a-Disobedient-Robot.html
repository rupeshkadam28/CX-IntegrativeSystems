
        <div class="container">
            <div class="content">
                <div class="row">
                    <div class="col-md-12 col-12 wysiwyg">
                      
                            <p>Imagine directing <a href="https://www.asme.org/engineering-topics/robotics"
                                    title="Robotics">your personal robot</a> to perform a task and getting a response
                                of: &ldquo;No, I can&rsquo;t do that.&rdquo;</p>

                            <p>Don&rsquo;t be surprised. As robots are integrated more and more into everyday life,
                                teaching them to &ldquo;disobey&rdquo; or question a directive is just as important, if
                                not more so, as teaching them to follow orders.</p>

                            <p>&ldquo;That&rsquo;s going to become a critical capability that all <a
                                    href="https://www.asme.org/engineering-topics/articles/robotics/robot-helps-people-get-dressed"
                                    title="Robot Helps People Get Dressed">instructable robots</a> will have to
                                have,&rdquo; says Matthias Scheutz, professor of computer science at Tufts University
                                School of Engineering and director of its Human-Robot Interaction Laboratory.</p>

                            <p><a href="https://aabme.asme.org/posts/asme-techcast-2-how-to-close-the-engineering-communication-gap"
                                    title="ASME Tech Cast">Listen to episode 2 of ASME TechCast</a><strong>:&nbsp; How
                                    Engineers Close the Communication Gap</strong></p>

                            <p>Scheutz is principal investigator on a Department of Defense-funded project, &ldquo;Moral
                                Competence in Computational Architectures for Robots,&rdquo; a collaboration with Brown
                                University and Rensselaer Polytechnic Institute. The researchers are exploring how to
                                equip robots with reasoning tools, a sense for right and wrong, and what the
                                consequences may be of their actions when facing real-world dilemmas, such as those that
                                may result in death versus life.</p>

                            <p>&ldquo;In situations that are morally charged, where there is possible conflict of
                                principles that you have to resolve, we want to understand whether robots are held to
                                the same standards [by society] as people are and what people even expect from
                                robots,&rdquo; Scheutz says.</p>

                            <p>Ultimately, researchers hope to design a computational architecture that allows robots <a
                                    href="https://www.asme.org/engineering-topics/articles/technology-and-society/designing-the-ethical-engineer"
                                    title="Designing the Ethical Engineer">to reason and act ethically</a> in such
                                situations. But Scheutz is quick to point out, &ldquo;This is not a project that will
                                have solved the problem at the end,&rdquo; he says. &ldquo;This is only a
                                beginning.&rdquo;</p>



                            
                                <figure class="secondary-image image-with-caption widget float-left figure">
                                    <img src="https://cdn.asme.org/getmedia/d157a6be-0d26-4ea8-bcf1-19353e0afe27/The-Moral-Beauty-of-a-Disobedient-Robot_01.jpg.aspx?width=340"
                                        class="figure-img img-fluid z-depth-1">
                                    <figcaption class="figure-caption caption text-right mt-3 mb-md-0 ml-0 col-md-auto">
                                        A network representation of prescribed actions in eight distinct scenes.
                                        Visualized using the Vibrant Data mappr tool. Image: Brown University
                                    </figcaption>
                                </figure>
                            
                            <p>RPI is leading the work on logical frameworks, how to reason through dilemma-like
                                situations, and Brown leads collecting empirical data, understanding norms, and what
                                expectations people have for robots. Scheutz&rsquo;s lab&rsquo;s main focus is
                                implementing the findings on the robotic platform, experimenting with algorithms and
                                exploring what people think of them.</p>

                            <p>Robots today can be given the wrong instructions, intentionally or not, that could have
                                harmful side effects to people, animals or property. These systems need to have an
                                algorithm that takes into account moral values and when an instruction might violate
                                those values, Scheutz says.</p>

                            <p>&ldquo;This concept has become very prominent in the context of autonomous driving when
                                an autonomous car has to make life and death decisions about who to run over,&rdquo;
                                Scheutz says.</p>

                            <p>If a driverless car &ldquo;sees&rdquo; a child run in front of the moving vehicle but
                                cannot brake in time, for example, the question becomes whether the car should move
                                forward or instead veer and crash into parked cars which could endanger the life of its
                                passenger but could be a lower risk for injury than almost certainly killing the child?
                            </p>

                            <p>&ldquo;That kind of decision-making is not available now,&rdquo; he says. &ldquo;In
                                autonomous cars, avoiding collisions is still the ultimate goal.&rdquo;</p>

                            <p>Or consider a car stopped at a red light and there is no cross traffic. A car is
                                approaching from behind at such a high speed that it could not stop before crashing into
                                the stopped car. What should the front car do? A human driver, seeing the rapidly
                                approaching car, would actively break the law and run the red light to avoid the
                                accident. &ldquo;There is no autonomous car now that can purposely break the law to
                                avoid an accident,&rdquo; he says.</p>

                            <p>Early findings gave researchers some insight into the psychological effects of agents and
                                autonomous agents making decisions. For example, do people believe it is morally right
                                or wrong for someone to take an action that would save the lives of several people while
                                killing fewer rather than taking no action and killing more (the so-called trolley
                                dilemma)? Additionally, do they have the same opinion when the action or inaction is
                                taken by a robot. The researchers found that most people put more blame on the person
                                for taking an action. In the same situation, though, the robot gets blamed more for not
                                taking an action.</p>

                            <p>The researchers continue to work on understanding the difference and under what
                                conditions opinions might be the same for both. They found that if the robot looks very
                                human-like or if those making the judgment are told that the robot really struggled with
                                the decision, then the difference of blame disappears.</p>

                            <p>&ldquo;Our hypothesis is that people subconsciously can simulate the robot&rsquo;s
                                decision-making dilemma better when it has a human-like appearance or when they are told
                                that it was struggling as a human would,&rdquo; Scheutz says.</p>

                            <p>One of the many engineering challenges in robotics is dealing with uncertainty, such as
                                deciding whether or not the robotic system is receiving the correct information from its
                                sensors. That uncertainty also needs to be taken into account in moral reasoning and
                                decision-making too.</p>

                            <p>&ldquo;We don&rsquo;t yet have a good way of factoring in uncertainty in comprehensive
                                normative reasoning. We have the logical descriptions of normative principles, and we
                                have a way of dealing with the specificity of the real world and uncertainty of the
                                sensors, but the approach we have proposed so far does not scale,&rdquo; Scheutz
                                says&ldquo;We can only handle a small number of conflicting norms. Yet, people have a
                                large number of norms.&rdquo;</p>

                            <p>Another challenge is with natural language processing. There are lots of ways people
                                express moral judgments, reprimand others, blame others and respond to blame in ways
                                that robots cannot. &ldquo;It&rsquo;s very likely that robots will be blamed because
                                they will screw up, and they need to be able to interact and be able to make their
                                justifications to people,&rdquo; he says &ldquo;We do not know how to do that
                                yet.&rdquo;</p>

                            <p>&ldquo;The bar is really high&rdquo; on laying the architectural foundation in the
                                robotic control system to take into account a whole moral range of ethical questions
                                where human norms apply, Schuetz says. Society expects people to abide by established
                                norms. When they don&rsquo;t, if a violation is illegal, they face consequences.</p>

                            <p>&ldquo;Current machines have no such notion, and yet they are being increasingly deployed
                                in human society and in social context where we are bringing into the interaction all of
                                these expectations,&rdquo; he adds.</p>

                            <p><em>Nancy Giges is an independent writer.</em></p>
                    </div>
                </div>

                <div class="row">
                    <div class="col-md-12 col-12 ">
                        <p><strong>Read More:</strong> </p>
                        <ul class="primary-list">
                            <li><a href="https://www.asme.org/engineering-topics/articles/manufacturing-design/breakthrough-makes-graphene-easier-3d-print"
                                    title="Breakthrough  Makes Graphene Easier to 3D Print">Breakthrough Makes Graphene
                                    Easier to 3D Print</a></li>
                            <li><a href="https://www.asme.org/engineering-topics/articles/manufacturing-design/making-sense-realtime-factory-data"
                                    title="Making  Sense ofÂ Real-Time Factory Data">Making Sense of&nbsp;Real-Time
                                    Factory Data</a></li>
                            <li><a href="https://www.asme.org/engineering-topics/articles/bioengineering/smart-bandage-does-it-all"
                                    title="Smart Bandage Does It All">Smart Bandage Does It All</a></li>
                        </ul>
                        
                    </div>
                </div>
            </div>
        </div>